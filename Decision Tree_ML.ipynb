{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "489598d4-4c5e-4556-aaa9-bb6fb2c292dc",
      "cell_type": "markdown",
      "source": "# Decision Trees \n\nA **Decision Tree** is a supervised learning algorithm used for **classification and regression**.  \nIt works by recursively splitting the data based on feature conditions to make predictions.\n\nEach split is a simple **if–else rule**, which makes decision trees highly interpretable.\n\n---\n\n## Machine Learning Application Flow\n\nA typical machine learning workflow (as shown in the diagram):\n\n1. Data Selection  \n2. Data Description (understanding features and target)  \n3. Exploratory Data Analysis (EDA)  \n4. Feature Engineering / Transformation  \n5. Algorithm Selection  \n6. Data Standardization / Normalization (optional for trees)  \n7. Train–Test Split  \n8. Model Training  \n9. Model Evaluation  \n10. Hyperparameter Tuning  \n11. Model Saving and Deployment  \n\nDecision Trees usually **do not require feature scaling**.\n\n---\n\n## Decision Tree – Real-Life Example\n\n**Problem:** Should you go out tonight?\n\n### Decision Flow Chart\n\n**Is time > 10 PM?**\n\n- **Yes** →  Don’t go  \n- **No** →\n  - **Do you have enough money?**\n    - **Yes** → Go and enjoy  \n    - **No** →  Don’t go\n\n",
      "metadata": {}
    },
    {
      "id": "a03256f8-8943-49b7-a99c-7a1095835d92",
      "cell_type": "markdown",
      "source": "\n- Each **question** is a node  \n- Each **answer** is a branch  \n- The **final decision** is a leaf node  \n\n---\n\n##  Decision Tree for Regression\n\nIn regression trees:\n\n- The feature space is divided into **non-overlapping regions**\n- The prediction in each region is the **mean of target values** in that region\n\nThe objective is to minimize the **Residual Sum of Squares (RSS)**:\n\n$$\n\\\n\\sum_{j=1}^{J} \\sum_{i \\in R_j} (y_i - \\hat{y}_{R_j})^2\n\\\n$$\n\nWhere:\n- $R_j$ = region  \n- $\\hat{y}_{R_j}$ = mean target value in region $j$\n\n---\n\n##  Recursive Binary Splitting (Greedy Approach)\n\nDecision trees use a **top-down greedy algorithm**:\n\n- At each step, the best split is chosen **locally**\n- The algorithm does **not look ahead** to future splits\n\nExample split:\n",
      "metadata": {}
    },
    {
      "id": "69d830f0-f38d-4c99-9efa-a070ce9c3644",
      "cell_type": "markdown",
      "source": "### Decision Rule\n\n**Is Age ≥ 35?**\n\n- **Yes**\n- **No**\n",
      "metadata": {}
    },
    {
      "id": "4c806b10-6af5-479c-8078-d2c1b764dc17",
      "cell_type": "markdown",
      "source": "\nThis split minimizes impurity (RSS, Gini, or Entropy) **at that step**.\n\n**Pros:** Fast and simple  \n**Cons:** Can lead to overfitting  \n\n---\n\n## Overfitting in Decision Trees\n\n- Deep trees memorize training data  \n- Low bias, **high variance**\n- Poor generalization to unseen data  \n\n**Solution:** Tree pruning\n\n---\n\n## Tree Pruning (Regularization)\n\nTree pruning reduces model complexity by penalizing tree size.\n\n### Cost-Complexity Pruning (Regression)\n\n$$\n\\sum (y_i - \\hat{y}_{R_m})^2 + \\alpha |T|\n$$\n\n**Where:**\n\n- $|T|$ = number of nodes  \n- $\\alpha$ = complexity penalty\n\n\n---\n\n## Types of Pruning\n\n### Pre-Pruning (Forward Pruning)\n\nStops tree growth early using conditions such as:\n- Maximum depth\n- Minimum samples per split\n- Minimum impurity decrease\n\n**Pros:** Faster  \n**Cons:** May underfit  \n\n---\n\n### Post-Pruning (Backward Pruning)\n\n- Grow full tree first\n- Remove branches using validation data\n\n**Pros:** Better generalization  \n**Cons:** Computationally expensive  \n\n---\n\n## Classification Trees (Brief)\n\n- Used when the target variable is categorical\n- Splits are chosen using:\n  - **Gini Impurity**\n  - **Entropy (Information Gain)**\n\nGoal: Make leaf nodes as **pure as possible**\n\n---\n\n## Why Decision Trees Are Popular\n\n- Easy to interpret  \n- Handles non-linear relationships  \n- No feature scaling required  \n- Works with numeric and categorical data  \n\n",
      "metadata": {}
    },
    {
      "id": "6c81f89b-6fdd-43cc-88e4-f5c2d4e23d79",
      "cell_type": "markdown",
      "source": "# Classification Trees\n\nRegression trees are used for **continuous (numeric)** targets, while **classification trees** are used for **categorical targets**.\n\nIn classification trees, node splitting is done using **impurity measures**, mainly:\n- Entropy\n- Information Gain\n- Gini Impurity\n\nThe goal at every split is to **reduce impurity** and make child nodes as **pure** as possible.\n",
      "metadata": {}
    },
    {
      "id": "ad06ace4-c1d6-43f5-aab8-4a741fb34611",
      "cell_type": "markdown",
      "source": "## Entropy\n\nEntropy measures the **randomness or impurity** in a dataset.\n\nWhen we split our nodes into two regions and put different observations in both the regions, the main goal is to reduce the entropy i.e. reduce the randomness in the region and divide our data cleanly than it was in the previous node. If splitting the node doesn’t lead into entropy reduction, we try to split based on a different condition, or we stop. \nA region is clean (low entropy) when it contains data with the same labels and random if there is a mixture of labels present (high entropy).\n\n- Low entropy → data is mostly one class (pure)\n- High entropy → data is mixed across classes\n\n### Formula (Binary Classification)\n\n$$\nE = -p \\log_2(p) - q \\log_2(q)\n$$\n\n**Where:**\n\n- $p$ = probability of class 1  \n- $q = 1 - p$ = probability of class 2\n",
      "metadata": {}
    },
    {
      "id": "9b7919f7-534a-4a60-abc8-d621e8d58691",
      "cell_type": "markdown",
      "source": "### Entropy Examples\n\n#### Case 1: Pure Node  \nAll samples belong to one class.\n\n- $p = 1,\\; q = 0$\n\n$$\nE = -(1)\\log_2(1) - (0)\\log_2(0) = 0\n$$\n\n **Entropy = 0 (perfectly pure)**\n\n---\n\n#### Case 2: Completely Mixed Node  \nEqual samples from both classes.\n\n- $p = 0.5,\\; q = 0.5$\n\n$$\nE = -0.5\\log_2(0.5) - 0.5\\log_2(0.5) = 1\n$$\n\n **uncertainty is maximum**",
      "metadata": {}
    },
    {
      "id": "7d112245-785d-465f-a9c8-090ab0133d74",
      "cell_type": "markdown",
      "source": "### Interpretation of Entropy\n\n| Data Distribution | Entropy |\n|------------------|---------|\n| All one class | 0 |\n| Mostly one class | Low |\n| Equal mix | High |\n\nDecision trees prefer splits that **reduce entropy the most**.",
      "metadata": {}
    },
    {
      "id": "1a8b57b1-20c1-4981-8cf2-fadb998f67c3",
      "cell_type": "markdown",
      "source": "## Information Gain\n\nInformation Gain measures **how much entropy decreases after a split**.\n\nInformation gain calculates the decrease in entropy after splitting a node. \nIt is the difference between entropies before and after the split. The more the information gain, the more entropy is removed.\n\nIt tells us **how good a feature is** for splitting.\n",
      "metadata": {}
    },
    {
      "id": "6889311f-12f0-429d-a607-4091e6da21c4",
      "cell_type": "markdown",
      "source": "### Formula\n\n$$\nIG(T, X) = Entropy(T) - Entropy(T \\mid X)\n$$\n\n**Where:**\n\n- $T$ = parent node  \n- $X$ = feature used to split\n",
      "metadata": {}
    },
    {
      "id": "f4314e9d-29db-4c02-91ab-67cc493572da",
      "cell_type": "markdown",
      "source": "### Information Gain Example\n\nSuppose we have 10 samples:\n- 6 Yes  \n- 4 No  \n\n**Parent entropy:**\n\n$$\nE(T) = -0.6\\log_2(0.6) - 0.4\\log_2(0.4) = 0.97\n$$\n\nNow split using a feature:\n\n**Left child:** 4 Yes, 1 No  \n\n$$\nE(L) = 0.72\n$$\n\n**Right child:** 2 Yes, 3 No  \n\n$$\nE(R) = 0.97\n$$\n\n**Weighted entropy after split:**\n\n$$\nE(T|X) = \\frac{5}{10}(0.72) + \\frac{5}{10}(0.97) = 0.845\n$$\n\n**Information Gain:**\n\n$$\nIG = 0.97 - 0.845 = 0.125\n$$\n\n**Higher Information Gain → Better split**\n",
      "metadata": {}
    },
    {
      "id": "2d63c207-2cc6-4d9c-87e6-6f3d58a45a6d",
      "cell_type": "markdown",
      "source": "## Gini Impurity\n\nGini impurity measures the **probability of incorrect classification** of a randomly chosen sample.\n\nGini impurity is a measure of how often a randomly chosen element from the set would be incorrectly labelled if it was randomly labelled according to the distribution of labels in the subset.’ \nIt is calculated by multiplying the probability that a given observation is classified into the correct class and sum of all the probabilities when that particular observation is classified into the wrong class\n\nIt is faster to compute than entropy and is commonly used in CART trees.\n",
      "metadata": {}
    },
    {
      "id": "6d993bc0-66ee-44aa-9ccd-c30c05ca0c29",
      "cell_type": "markdown",
      "source": "### Formula (Multi-class)\n\n$$\nGini = 1 - \\sum_{i=1}^{k} p_i^2\n$$\n\n**Where:**\n\n- $p_i$ = probability of class $i$  \n- $k$ = number of classes\n",
      "metadata": {}
    },
    {
      "id": "d3866ee0-4f9c-4184-9076-081518582f75",
      "cell_type": "markdown",
      "source": "### Gini Impurity Examples\n\n#### Case 1: Pure Node  \nAll samples belong to one class.\n\n- $p_1 = 1$\n\n$$\nGini = 1 - (1)^2 = 0\n$$\n\n**No impurity**\n\n---\n\n#### Case 2: Two classes equally mixed\n\n- $p_1 = 0.5,\\; p_2 = 0.5$\n\n$$\nGini = 1 - (0.25 + 0.25) = 0.5\n$$\n\n**High impurity**\n",
      "metadata": {}
    },
    {
      "id": "665ca55e-2c06-44bc-9212-89872b161765",
      "cell_type": "markdown",
      "source": "### Entropy vs Gini (Quick Comparison)\n\n| Measure | Entropy | Gini |\n|-------|--------|------|\n| Range | 0 to 1 | 0 to 0.5 (binary) |\n| Computation | Log-based (slower) | Squared terms (faster) |\n| Sensitivity | More sensitive to changes | Slightly less sensitive |\n| Used in | ID3, C4.5 | CART (sklearn default) |\n",
      "metadata": {}
    },
    {
      "id": "b4576488-11a1-4d90-b782-335138ef02fd",
      "cell_type": "markdown",
      "source": "## Key Takeaways\n\n- Entropy measures **disorder**\n- Information Gain measures **reduction in disorder**\n- Gini measures **misclassification probability**\n- Decision trees always choose splits that **minimize impurity**",
      "metadata": {}
    },
    {
      "id": "e43268d8-7d5b-4fc1-b428-637550b119e9",
      "cell_type": "markdown",
      "source": "### Gini Impurity Interpretation\n\nA node is called **\"quite impure\"** when its **Gini value is high**, meaning the samples are spread across many classes.\n\n| Gini Value | Meaning |\n|-----------|---------|\n| **0.0** | Perfectly pure |\n| **0.1 – 0.3** | Mostly pure |\n| **0.3 – 0.5** | Moderately impure |\n| **> 0.5** | **Highly / Quite impure** |\n",
      "metadata": {}
    },
    {
      "id": "f7bf6231-c620-44e1-b7eb-b9570576f096",
      "cell_type": "markdown",
      "source": "# Different Algorithms for Decision Tree\n\nThere are multiple algorithms used to construct Decision Trees. Each algorithm differs mainly in:\n- The **splitting criterion**\n- The **type of data** it supports\n- Whether it supports **classification, regression, or both**\n",
      "metadata": {}
    },
    {
      "id": "8ea7b151-f417-43bf-8a90-857e985e9439",
      "cell_type": "markdown",
      "source": "## 1. ID3 (Iterative Dichotomiser 3)\n\nID3 is one of the earliest algorithms used to build **decision trees for classification**.\n\n### Key Characteristics:\n- Uses **Information Gain** as the splitting criterion\n- Works **only with categorical features**\n- Does **not support continuous variables**\n- Produces **multi-way splits**\n\n### Limitation:\nID3 cannot handle numeric data directly and does not support pruning, which makes it prone to **overfitting**.\n",
      "metadata": {}
    },
    {
      "id": "76f5232d-c6e7-4644-9b38-19201636dde2",
      "cell_type": "markdown",
      "source": "## 2. C4.5\n\nC4.5 is an **improved and extended version of ID3**.\n\n### Key Characteristics:\n- Supports **both categorical and continuous features**\n- Uses **Information Gain Ratio** (a normalized version of Information Gain)\n- Handles **missing values**\n- Supports **tree pruning**\n\n### Advantage over ID3:\nC4.5 reduces bias toward attributes with many values and is more robust for real-world datasets.",
      "metadata": {}
    },
    {
      "id": "2443e50a-63ef-4b1d-8b0d-1205a13142b8",
      "cell_type": "markdown",
      "source": "## 3. CART (Classification and Regression Trees)\n\nCART is the **most widely used decision tree algorithm**.\n\n### Key Characteristics:\n- Uses **Gini Impurity** as the default splitting criterion\n- Can also use **Entropy** for classification\n- Supports **both classification and regression**\n- Produces **binary splits only**\n- Used in popular libraries like **scikit-learn**\n\nBecause Gini impurity is computationally cheaper than entropy, CART uses it by default.",
      "metadata": {}
    },
    {
      "id": "75c8f215-f55e-454b-b50c-2a8f6a5ef06b",
      "cell_type": "markdown",
      "source": "## Advantages of Decision Trees\n\n- Can be used for **both classification and regression**\n- Easy to understand and interpret\n- Rules are clearly visible in the tree structure\n- No need for **feature scaling or normalization**\n- Handles non-linear relationships well\n\n## Disadvantages of Decision Trees\n\n- Highly sensitive to small changes in data\n- Greedy nature can lead to **sub-optimal splits**\n- High risk of **overfitting**\n- Training can be slower compared to simpler models",
      "metadata": {}
    },
    {
      "id": "7630f591-f85f-4479-9337-b6959ee55dda",
      "cell_type": "markdown",
      "source": "# Cross-Validation\n\nSuppose you train a model on a dataset and evaluate it using the **same training data**.  \nYou might get very high accuracy (95% or even 100%).\n\n**Is the model reliable?**  \n**No.** This usually means the model has **overfitted** the training data and may perform poorly on unseen data.\n\n**Cross-validation** is used to estimate how well a model generalizes to new data.\n\n**Cross-validation** is a technique used to check how well a machine-learning model will perform on **new, unseen data**.\n\nInstead of testing the model on the same data it was trained on, we repeatedly split the data into **training** and **testing** parts and evaluate performance.\n\nThis helps avoid **overfitting**.\n\n\n---\n\n## Why Cross-Validation?\n\n- Training accuracy alone is misleading  \n- Helps detect **overfitting**  \n- Provides a more reliable estimate of model performance  \n- Helps in **model selection and hyperparameter tuning**\n\n---\n\n## 1. Hold-Out Method\n\nThe dataset is split into **two parts**:\n- **Training set**\n- **Test set**\n\n### Steps:\n1. Train the model on the training set  \n2. Test the model on the test set  \n3. Measure performance (accuracy, error, etc.)\n\n### Pros:\n- Simple and fast  \n- Computationally inexpensive  \n\n### Cons:\n- High variance  \n- Performance depends heavily on how data is split  \n\n---\n\n## 2. k-Fold Cross-Validation\n\nThe dataset is divided into **k equal parts (folds)**.\n\n### Steps:\n1. Use 1 fold as **test data**  \n2. Use remaining **k−1 folds as training data**  \n3. Repeat this process **k times**  \n4. Each fold is used once as test data  \n5. Final performance is the **average error**\n\n### Mathematical Form:\n\n$$\nCV(k) = \\frac{1}{k} \\sum_{i=1}^{k} \\text{Error}_i\n$$\n\n(For regression, error is often **MSE**)\n\n### Pros:\n- Lower variance than hold-out  \n- Uses all data for training and testing  \n\n### Cons:\n- Computationally expensive  \n- Model is trained **k times**\n\n---\n\n## 3. Leave-One-Out Cross-Validation (LOOCV)\n\nA special case of k-fold where:\n\n$$\nk = n \\quad (\\text{number of samples})\n$$\n\n### Steps:\n1. Use **1 data point** as test data  \n2. Use remaining **n−1 points** as training data  \n3. Repeat for all data points  \n4. Average all errors  \n\n### Pros:\n- Very low bias  \n- Maximum use of data  \n\n### Cons:\n- Extremely expensive computationally  \n- High variance in practice  \n\n---\n\n## Comparison Summary\n\n| Method | Bias | Variance | Computation |\n|------|------|----------|-------------|\n| Hold-Out | High | High | Low |\n| k-Fold | Medium | Medium | Medium |\n| LOOCV | Low | High | Very High |\n",
      "metadata": {}
    },
    {
      "id": "beac0958-d478-491c-888a-164b2f1db5e7",
      "cell_type": "markdown",
      "source": "# Bias–Variance Tradeoff in Cross-Validation  \n*(Hold-Out, k-Fold CV, LOOCV)*\n\nCross-validation is used to **estimate how well a model will perform on unseen data**.  \nDifferent validation methods produce **different bias and variance** in this estimate.\n\n---\n\n## What is an Estimate?\n\n- **Estimate** = our **guess of the model’s true test error**\n- True test error = performance on future data (unknown)\n\n---\n\n## Bias and Variance (Simple Meaning)\n\n### Bias\n- Bias asks: **Is my estimate systematically wrong?**\n- High bias → estimate far from true error  \n- Low bias → estimate close to true error\n\n### Variance\n- Variance asks: **Does my estimate change a lot with different data splits?**\n- High variance → unstable estimate  \n- Low variance → stable estimate\n\n---\n\n## Hold-Out Validation\n\n### How it works\n- Split data once into:\n  - Training set\n  - Test set\n\n### Bias\n- Model trained on less data\n- Model is weaker → error looks worse\n- **Bias is high**\n\n### Variance\n- Different splits give very different results\n- **Variance is high**\n\n---\n\n## Leave-One-Out Cross-Validation (LOOCV)\n\n### How it works\n- Dataset has **n samples**\n- Train **n models**\n- Each time:\n  - 1 sample → test\n  - Remaining **n−1 samples** → train\n- Average all errors\n\n### Bias\n- Training set is almost the full dataset\n- Very close to true performance\n- **Bias is very low**\n\n### Variance\n- Training sets are almost identical\n- Errors are highly correlated\n- **Variance is high**\n\n---\n\n## k-Fold Cross-Validation (k = 5 or 10)\n\n### How it works\n- Split data into **k folds**\n- Train **k models**\n- Each fold used once as test data\n- Average the errors\n\n### Bias\n- Training size is large but not full\n- **Bias is medium**\n\n### Variance\n- Training sets differ by whole folds\n- Errors are less correlated\n- **Variance is medium**\n\n---\n\n## Final Takeaway\n\n- **Hold-Out** → Fast but unreliable  \n- **LOOCV** → Accurate on average but unstable  \n- **k-Fold CV ** → **Best balance of bias and variance**\n",
      "metadata": {}
    },
    {
      "id": "ab42b034-49a5-40ce-bcf1-ade2f81046a6",
      "cell_type": "markdown",
      "source": "# What Are Hyperparameters? \n\n**Hyperparameters** are the settings you choose **before training** a model.  \nThey control **how the decision tree grows**, not what it learns from data.\n\nThink of hyperparameters like **rules for building the tree**, not the data itself.\n\n---\n\n## Why Do We Need Hyperparameters?\n\n- Prevent **overfitting** (tree becoming too complex)\n- Control **tree size**\n- Improve **generalization**\n- Balance **bias vs variance**",
      "metadata": {}
    },
    {
      "id": "0a24fe00-4c15-4278-8a0b-1517125a542d",
      "cell_type": "markdown",
      "source": "#### Parameters\n  ----------\n * criterion : string, optional (default=\"gini\")\n       The function to measure the quality of a split. Supported criteria are\n       \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n   \n *  splitter : string, optional (default=\"best\")\n       The strategy used to choose the split at each node. Supported\n       strategies are \"best\" to choose the best split and \"random\" to choose\n       the best random split.\n   \n *  max_depth : int or None, optional (default=None)\n       The maximum depth of the tree. If None, then nodes are expanded until\n       all leaves are pure or until all leaves contain less than\n       min_samples_split samples.\n   \n *  min_samples_split : int, float, optional (default=2)\n       The minimum number of samples required to split an internal node:\n   \n       - If int, then consider `min_samples_split` as the minimum number.\n       - If float, then `min_samples_split` is a fraction and\n         `ceil(min_samples_split * n_samples)` are the minimum\n         number of samples for each split.\n   \n       .. versionchanged:: 0.18\n          Added float values for fractions.\n   \n *  min_samples_leaf : int, float, optional (default=1)\n       The minimum number of samples required to be at a leaf node.\n       A split point at any depth will only be considered if it leaves at\n       least ``min_samples_leaf`` training samples in each of the left and\n       right branches.\n\n*  random_state : int, RandomState instance or None, optional (default=None)\n       If int, random_state is the seed used by the random number generator;\n       If RandomState instance, random_state is the random number generator;\n       If None, the random number generator is the RandomState instance used\n       by `np.random`.\n   \n *  max_leaf_nodes : int or None, optional (default=None)\n       Grow a tree with ``max_leaf_nodes`` in best-first fashion.\n       Best nodes are defined as relative reduction in impurity.\n       If None then unlimited number of leaf nodes.\n   \n *  min_impurity_decrease : float, optional (default=0.)\n       A node will be split if this split induces a decrease of the impurity\n       greater than or equal to this value.\n   \n *  min_impurity_split : float, (default=1e-7)\n       Threshold for early stopping in tree growth. A node will split\n       if its impurity is above the threshold, otherwise it is a leaf.\n       \n *  class_weight : dict, list of dicts, \"balanced\" or None, default=None\n       Weights associated with classes in the form ``{class_label: weight}``.\n       If not given, all classes are supposed to have weight one. For\n       multi-output problems, a list of dicts can be provided in the same\n       order as the columns of y.\n   \n * presort : bool, optional (default=False)\n       Whether to presort the data to speed up the finding of best splits in\n       fitting. For the default settings of a decision tree on large\n       datasets, setting this to true may slow down the training process.\n       When using either a smaller dataset or a restricted depth, this may\n       speed up the training.\n   \n\nWhen we do hyperparameter tuning, we basically try to find those sets and values of hyperparameters which will give us a model with maximum accuracy.\n",
      "metadata": {}
    },
    {
      "id": "ef46cbf6-5903-4e9e-8d23-2822e48f8d44",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}